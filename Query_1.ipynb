{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>125</td><td>application_1738075734771_0126</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0126/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0126_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>126</td><td>application_1738075734771_0127</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0127/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0127_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>123</td><td>application_1738075734771_0124</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0124/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-137.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0124_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>124</td><td>application_1738075734771_0125</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0125/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0125_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>126</td><td>application_1738075734771_0127</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0127/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0127_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Master: yarn\n",
      "Executor Instances: 4"
     ]
    }
   ],
   "source": [
    "master = spark.sparkContext.master\n",
    "print(f\"Spark Master: {master}\")\n",
    "\n",
    "conf = spark.sparkContext.getConf()\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "\n",
    "data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2uaG6GoZthpT",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Load the dataset with header and infer schema\n",
    "data_1 = spark.read.csv(\n",
    "    path=data_path_1,\n",
    "    header=True,          # Use header row for column names\n",
    "    inferSchema=True      # Let Spark infer the schema\n",
    ").select(\"Vict Age\", \"Crm Cd Desc\")\n",
    "\n",
    "data_2 = spark.read.csv(\n",
    "    path=data_path_2,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").select(\"Vict Age\", \"Crm Cd Desc\")\n",
    "\n",
    "data_1.show()\n",
    "data_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "def count_aggravated_assaults_by_age_group(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filters rows where 'Crm Cd Desc' contains 'AGGRAVATED ASSAULT' and calculates the\n",
    "    sum of incidents per age group.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input PySpark DataFrame with 'Crm Cd Desc' and 'Vict Age' columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with 'Age Group' and the count of aggravated assaults per group.\n",
    "    \"\"\"\n",
    "    # Step 1: Filter rows with \"AGGRAVATED ASSAULT\" in the 'Crm Cd Desc' column\n",
    "    filtered_df = df.filter(col(\"Crm Cd Desc\").like(\"%AGGRAVATED ASSAULT%\"))\n",
    "    \n",
    "    # Step 2: Create a new column for age groups\n",
    "    categorized_df = filtered_df.withColumn(\n",
    "        \"Age Group\",\n",
    "        when(col(\"Vict Age\") < 18, \"Children (<18)\")\n",
    "        .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults (18-24)\")\n",
    "        .when((col(\"Vict Age\") > 24) & (col(\"Vict Age\") < 65), \"Adults (25-64)\")\n",
    "        .when(col(\"Vict Age\") >= 65, \"Elderly (65+)\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    "    \n",
    "    # Step 3: Group by age group and count incidents\n",
    "    result = categorized_df.groupBy(\"Age Group\").count().orderBy(col(\"count\").desc())\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function for both datasets\n",
    "result_1 = count_aggravated_assaults_by_age_group(data_1)\n",
    "result_2 = count_aggravated_assaults_by_age_group(data_2)\n",
    "\n",
    "result_1.show()\n",
    "result_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           Age Group|count|\n",
      "+--------------------+-----+\n",
      "|      Adults (25-64)|72610|\n",
      "|Young Adults (18-24)|23472|\n",
      "|      Children (<18)|10724|\n",
      "|       Elderly (65+)| 3099|\n",
      "|      Adults (25-64)|48483|\n",
      "|Young Adults (18-24)|10133|\n",
      "|      Children (<18)| 5204|\n",
      "|       Elderly (65+)| 2886|\n",
      "+--------------------+-----+\n",
      "\n",
      "Final Result:\n",
      "+--------------------+-----------+\n",
      "|           Age Group|total_count|\n",
      "+--------------------+-----------+\n",
      "|      Adults (25-64)|     121093|\n",
      "|Young Adults (18-24)|      33605|\n",
      "|      Children (<18)|      15928|\n",
      "|       Elderly (65+)|       5985|\n",
      "+--------------------+-----------+\n",
      "\n",
      "Execution time: 23.36 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df = result_1.union(result_2)\n",
    "combined_df.show()\n",
    "\n",
    "# Group by age group and sum the counts\n",
    "result = combined_df \\\n",
    "    .groupBy(\"Age Group\") \\\n",
    "    .sum(\"count\") \\\n",
    "    .withColumnRenamed(\"sum(count)\", \"total_count\") \\\n",
    "    .orderBy(col(\"total_count\").desc())\n",
    "\n",
    "print(\"Final Result:\") \n",
    "result.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RDD API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The csv parser is used because:\n",
    "- It correctly interprets commas within quoted fields, ensuring that these commas are treated as part of the field and not as delimiters. (We had an issue with the Crime Description quote and the commas inside)\n",
    "\n",
    "- It handles cases where some rows might have missing or extra fields.\n",
    "\n",
    "- It supports escaping characters, such as double quotes within a quoted field (\"field, with \\\"quotes\\\" and commas\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 'VIOLATION OF COURT ORDER'], [0, 'VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)'], [0, 'OTHER MISCELLANEOUS CRIME'], [47, 'VIOLATION OF COURT ORDER'], [47, 'RAPE, ATTEMPTED'], [23, 'SHOPLIFTING - PETTY THEFT ($950 & UNDER)'], [46, 'BURGLARY FROM VEHICLE'], [51, 'ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT'], [30, 'ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT'], [55, 'THEFT-GRAND ($950.01 & OVER)EXCPT,GUNS,FOWL,LIVESTK,PROD']]\n",
      "[[0, 'VEHICLE - STOLEN'], [47, 'BURGLARY FROM VEHICLE'], [19, 'BIKE - STOLEN'], [19, 'SHOPLIFTING-GRAND THEFT ($950.01 & OVER)'], [28, 'THEFT OF IDENTITY'], [41, 'THEFT OF IDENTITY'], [25, 'THEFT OF IDENTITY'], [27, 'THEFT OF IDENTITY'], [24, 'THEFT OF IDENTITY'], [26, 'BATTERY - SIMPLE ASSAULT']]"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "import csv\n",
    "\n",
    "# Load the CSV file\n",
    "rdd_1 = sc.textFile(data_path_1).map(lambda x: list(csv.reader([x]))[0])\n",
    "rdd_2 = sc.textFile(data_path_2).map(lambda x: list(csv.reader([x]))[0])\n",
    "\n",
    "# Get the header row\n",
    "header_1 = rdd_1.first()\n",
    "header_2 = rdd_2.first()\n",
    "\n",
    "# Remove the header and keep relevant info\n",
    "rdd_1 = rdd_1.filter(lambda row: row != header_1).map(lambda x: [int(x[11]), x[9]])\n",
    "rdd_2 = rdd_2.filter(lambda row: row != header_2).map(lambda x: [int(x[11]), x[9]])\n",
    "\n",
    "# print(rdd_1.take(10))\n",
    "# print(rdd_2.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def process_rdd_for_aggravated_assaults(rdd):\n",
    "    \"\"\"\n",
    "    Processes an RDD to calculate the total number of aggravated assaults per age group.\n",
    "\n",
    "    Parameters:\n",
    "    rdd (RDD): Input RDD with [age, crime_description].\n",
    "\n",
    "    Returns:\n",
    "    RDD: RDD with total counts of aggravated assaults per age group.\n",
    "    \"\"\"\n",
    "    # Filter for rows containing 'aggravated assault'\n",
    "    filtered_rdd = rdd.filter(lambda row: \"aggravated assault\" in row[1].lower())\n",
    "\n",
    "    # Map to age groups\n",
    "    age_grouped_rdd = filtered_rdd.map(lambda row: (\n",
    "        \"Children (<18)\" if row[0] < 18 else\n",
    "        \"Young Adults (18-24)\" if 18 <= row[0] <= 24 else\n",
    "        \"Adults (25-64)\" if 25 <= row[0] <= 64 else\n",
    "        \"Elderly (65+)\",\n",
    "        1  # Each row represents one crime\n",
    "    ))\n",
    "\n",
    "    # Count total crimes per age group\n",
    "    result_rdd = age_grouped_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    return result_rdd\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_rdd_1 = process_rdd_for_aggravated_assaults(rdd_1)\n",
    "result_rdd_2 = process_rdd_for_aggravated_assaults(rdd_2)\n",
    "\n",
    "# print(result_rdd_1.sortByKey().take(5))\n",
    "# print(result_rdd_2.sortByKey().take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "[('Adults (25-64)', 121093), ('Young Adults (18-24)', 33605), ('Children (<18)', 15928), ('Elderly (65+)', 5985)]\n",
      "Execution time: 28.31 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Combine the results and sort the final rdd\n",
    "combined_rdd = result_rdd_1 \\\n",
    "    .union(result_rdd_2) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Final Result:\")\n",
    "print(combined_rdd.collect())\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
