{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>129</td><td>application_1738075734771_0130</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-12.eu-central-1.compute.internal:20888/proxy/application_1738075734771_0130/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-139.eu-central-1.compute.internal:8042/node/containerlogs/container_1738075734771_0130_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Query 2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run this to load data from csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time_csv = time.time()\n",
    "\n",
    "crime_data_1 = spark.read.csv( \\\n",
    "    path=crime_data_path_1, \\\n",
    "    header=True, \\\n",
    "    inferSchema=True\n",
    ").select(\"DATE OCC\", \"AREA NAME\", \"Status Desc\")\n",
    "\n",
    "crime_data_2 = spark.read.csv(\n",
    "    path=crime_data_path_2,\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").select(\"DATE OCC\", \"AREA NAME\", \"Status Desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run this to load data from parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time_parquet = time.time()\n",
    "\n",
    "# Define the path to the Parquet file on S3\n",
    "s3_bucket_path = \"s3://groups-bucket-dblab-905418150721/group34/Query_2/\"\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "parquet_df = spark.read.parquet(s3_bucket_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "crimes = crime_data_1.union(crime_data_2)\n",
    "\n",
    "# crimes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The Window.partitionBy(\"Year\").orderBy(col(\"Closed_Case_Rate\").desc()) ensures that the ranking is applied within each year, ordering by Closed_Case_Rate in descending order.\n",
    " - dense_rank: Assigns ranks starting from 1 for each year, without skipping numbers in case of ties.\n",
    " - Column Addition: The .withColumn(\"Rank\", dense_rank().over(window_spec)) adds the rank column.\n",
    " - Ordering: Finally, orderBy(col(\"Year\"), col(\"Rank\")) ensures the results are displayed sequentially by year and rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----+\n",
      "|AREA NAME| Status Desc|Year|\n",
      "+---------+------------+----+\n",
      "|   Newton|Adult Arrest|2010|\n",
      "|  Pacific| Invest Cont|2010|\n",
      "|   Newton| Invest Cont|2010|\n",
      "|Hollywood| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central|Adult Arrest|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central|Adult Arrest|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Adult Other|2010|\n",
      "|  Central|Adult Arrest|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central| Invest Cont|2010|\n",
      "|  Central|Adult Arrest|2010|\n",
      "+---------+------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Final Result:\n",
      "+----+-----------+------------------+----+\n",
      "|Year|  AREA NAME|  Closed_Case_Rate|Rank|\n",
      "+----+-----------+------------------+----+\n",
      "|2010|    Rampart| 32.84713448949121|   1|\n",
      "|2010|    Olympic|31.515289821999087|   2|\n",
      "|2010|     Harbor| 29.36028339237341|   3|\n",
      "|2011|    Olympic|35.040060090135206|   1|\n",
      "|2011|    Rampart|  32.4964471814306|   2|\n",
      "|2011|     Harbor| 28.51336246316431|   3|\n",
      "|2012|    Olympic| 34.29708533302119|   1|\n",
      "|2012|    Rampart| 32.46000463714352|   2|\n",
      "|2012|     Harbor|29.509585848956675|   3|\n",
      "|2013|    Olympic| 33.58217940999398|   1|\n",
      "|2013|    Rampart|  32.1060382916053|   2|\n",
      "|2013|     Harbor|29.723638951488557|   3|\n",
      "|2014|   Van Nuys|  32.0215235281705|   1|\n",
      "|2014|West Valley| 31.49754809505847|   2|\n",
      "|2014|    Mission|31.224939855653567|   3|\n",
      "|2015|   Van Nuys|32.265140677157845|   1|\n",
      "|2015|    Mission|30.463762673676303|   2|\n",
      "|2015|   Foothill|30.353001803658852|   3|\n",
      "|2016|   Van Nuys|32.194518462124094|   1|\n",
      "|2016|West Valley| 31.40146437042384|   2|\n",
      "|2016|   Foothill|29.908647228131645|   3|\n",
      "|2017|   Van Nuys|  32.0554272517321|   1|\n",
      "|2017|    Mission|31.055387158996968|   2|\n",
      "|2017|   Foothill|30.469700657094183|   3|\n",
      "|2018|   Foothill|30.731346958877126|   1|\n",
      "|2018|    Mission|30.727023319615913|   2|\n",
      "|2018|   Van Nuys|28.905206942590123|   3|\n",
      "|2019|    Mission|30.727411112319235|   1|\n",
      "|2019|West Valley| 30.57974335472044|   2|\n",
      "|2019|N Hollywood| 29.23808669119627|   3|\n",
      "|2020|West Valley|30.771131982204647|   1|\n",
      "|2020|    Mission| 30.14974649215894|   2|\n",
      "|2020|     Harbor|29.693486590038315|   3|\n",
      "|2021|    Mission|30.318115590092276|   1|\n",
      "|2021|West Valley|28.971087440009363|   2|\n",
      "|2021|   Foothill|27.993757094211126|   3|\n",
      "|2022|West Valley|26.536367172306498|   1|\n",
      "|2022|     Harbor|26.337538060026098|   2|\n",
      "|2022|    Topanga|26.234013317831096|   3|\n",
      "|2023|   Foothill| 26.76076020122974|   1|\n",
      "|2023|    Topanga|26.538022616453986|   2|\n",
      "|2023|    Mission|25.662731120516817|   3|\n",
      "|2024|N Hollywood|19.598528961078763|   1|\n",
      "|2024|   Foothill|18.620882188721385|   2|\n",
      "|2024|77th Street|17.586318167150694|   3|\n",
      "+----+-----------+------------------+----+\n",
      "\n",
      "Execution time: 6.31 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, year, count, when, sum, round\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "import time\n",
    "\n",
    "crimes = crimes \\\n",
    "    .withColumn(\"DATE OCC\", to_timestamp(col(\"DATE OCC\"), \"MM/dd/yyyy hh:mm:ss a\")) \\\n",
    "    .withColumn(\"Year\", year(col(\"DATE OCC\"))) \\\n",
    "    .drop(\"DATE OCC\") \\\n",
    "\n",
    "crimes.show()\n",
    "\n",
    "crimes_grouped = crimes \\\n",
    "    .groupBy(\"Year\", \"AREA NAME\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Total_Crimes\"), # Total crimes per year per police station\n",
    "        sum(when(~((col(\"Status Desc\") == \"UNK\") | (col(\"Status Desc\") == \"Invest Cont\")), 1).otherwise(0))\n",
    "        .alias(\"Closed_Crimes\") # Count of closed crimes\n",
    "    ) \\\n",
    "    .withColumn(\"Closed_Case_Rate\", (col(\"Closed_Crimes\") / col(\"Total_Crimes\"))*100) \\\n",
    "\n",
    "window_spec = Window.partitionBy(\"Year\").orderBy(col(\"Closed_Case_Rate\").desc())\n",
    "\n",
    "top_crimes = crimes_grouped \\\n",
    "    .drop(\"Total_Crimes\", \"Closed_Crimes\") \\\n",
    "    .withColumn(\"Rank\", dense_rank().over(window_spec)) \\\n",
    "    .filter(col(\"Rank\") <= 3) \\\n",
    "    .orderBy(col(\"Year\"), col(\"Rank\"))\n",
    "\n",
    "print(\"Final Result:\")\n",
    "top_crimes.show(100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Combine the datasets\n",
    "crimes = crime_data_1.union(crime_data_2)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crimes.createOrReplaceTempView(\"crimes\")\n",
    "\n",
    "# Display the table (optional)\n",
    "# crimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result:\n",
      "+----+-----------+-----------------+----+\n",
      "|Year|  AREA NAME| Closed_Case_Rate|Rank|\n",
      "+----+-----------+-----------------+----+\n",
      "|2010|    Rampart|32.84713448949121|   1|\n",
      "|2010|    Olympic|31.51528982199909|   2|\n",
      "|2010|     Harbor|29.36028339237341|   3|\n",
      "|2011|    Olympic|35.04006009013520|   1|\n",
      "|2011|    Rampart|32.49644718143060|   2|\n",
      "|2011|     Harbor|28.51336246316431|   3|\n",
      "|2012|    Olympic|34.29708533302119|   1|\n",
      "|2012|    Rampart|32.46000463714352|   2|\n",
      "|2012|     Harbor|29.50958584895668|   3|\n",
      "|2013|    Olympic|33.58217940999398|   1|\n",
      "|2013|    Rampart|32.10603829160530|   2|\n",
      "|2013|     Harbor|29.72363895148855|   3|\n",
      "|2014|   Van Nuys|32.02152352817050|   1|\n",
      "|2014|West Valley|31.49754809505847|   2|\n",
      "|2014|    Mission|31.22493985565357|   3|\n",
      "|2015|   Van Nuys|32.26514067715784|   1|\n",
      "|2015|    Mission|30.46376267367630|   2|\n",
      "|2015|   Foothill|30.35300180365885|   3|\n",
      "|2016|   Van Nuys|32.19451846212410|   1|\n",
      "|2016|West Valley|31.40146437042384|   2|\n",
      "|2016|   Foothill|29.90864722813165|   3|\n",
      "|2017|   Van Nuys|32.05542725173210|   1|\n",
      "|2017|    Mission|31.05538715899697|   2|\n",
      "|2017|   Foothill|30.46970065709418|   3|\n",
      "|2018|   Foothill|30.73134695887712|   1|\n",
      "|2018|    Mission|30.72702331961591|   2|\n",
      "|2018|   Van Nuys|28.90520694259012|   3|\n",
      "|2019|    Mission|30.72741111231923|   1|\n",
      "|2019|West Valley|30.57974335472044|   2|\n",
      "|2019|N Hollywood|29.23808669119627|   3|\n",
      "|2020|West Valley|30.77113198220465|   1|\n",
      "|2020|    Mission|30.14974649215894|   2|\n",
      "|2020|     Harbor|29.69348659003831|   3|\n",
      "|2021|    Mission|30.31811559009228|   1|\n",
      "|2021|West Valley|28.97108744000936|   2|\n",
      "|2021|   Foothill|27.99375709421112|   3|\n",
      "|2022|West Valley|26.53636717230650|   1|\n",
      "|2022|     Harbor|26.33753806002610|   2|\n",
      "|2022|    Topanga|26.23401331783110|   3|\n",
      "|2023|   Foothill|26.76076020122974|   1|\n",
      "|2023|    Topanga|26.53802261645399|   2|\n",
      "|2023|    Mission|25.66273112051682|   3|\n",
      "|2024|N Hollywood|19.59852896107876|   1|\n",
      "|2024|   Foothill|18.62088218872138|   2|\n",
      "|2024|77th Street|17.58631816715069|   3|\n",
      "+----+-----------+-----------------+----+\n",
      "\n",
      "Execution time for SQL API: 2.16 seconds\n",
      "Execution time for CSV import and SQL API: 4.39 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Step 1: Create formatted_crimes\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW formatted_crimes AS\n",
    "    SELECT \n",
    "        TO_DATE(CAST(UNIX_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a') AS TIMESTAMP)) AS `DATE OCC`,\n",
    "        YEAR(TO_DATE(CAST(UNIX_TIMESTAMP(`DATE OCC`, 'MM/dd/yyyy hh:mm:ss a') AS TIMESTAMP))) AS Year,\n",
    "        `AREA NAME`,\n",
    "        `Status Desc`\n",
    "    FROM crimes\n",
    "\"\"\")\n",
    "\n",
    "# formatted_crimes = spark.sql(\"SELECT * FROM formatted_crimes\")\n",
    "# formatted_crimes.show()\n",
    "\n",
    "# Step 2: Create aggregated_crimes\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW aggregated_crimes AS\n",
    "    SELECT \n",
    "        Year,\n",
    "        `AREA NAME`,\n",
    "        COUNT(*) AS Total_Crimes,\n",
    "        SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS Closed_Crimes,\n",
    "        Closed_Crimes * 100.0 / COUNT(*) AS Closed_Case_Rate\n",
    "    FROM formatted_crimes\n",
    "    GROUP BY Year, `AREA NAME`\n",
    "\"\"\")\n",
    "\n",
    "# aggregated_crimes = spark.sql(\"SELECT * FROM aggregated_crimes\")\n",
    "# aggregated_crimes.show()\n",
    "\n",
    "# Step 3: Create ranked_crimes\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW ranked_crimes AS\n",
    "    SELECT \n",
    "        Year,\n",
    "        `AREA NAME`,\n",
    "        Closed_Case_Rate,\n",
    "        DENSE_RANK() OVER (PARTITION BY Year ORDER BY Closed_Case_Rate DESC) AS Rank\n",
    "    FROM aggregated_crimes\n",
    "\"\"\")\n",
    "\n",
    "# ranked_crimes = spark.sql(\"SELECT * FROM ranked_crimes\")\n",
    "# ranked_crimes.show()\n",
    "\n",
    "# Step 4: Execute final query\n",
    "top_crimes = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        `AREA NAME`,\n",
    "        Closed_Case_Rate,\n",
    "        Rank\n",
    "    FROM ranked_crimes\n",
    "    WHERE Rank <= 3\n",
    "    ORDER BY Year, Rank\n",
    "\"\"\")\n",
    "\n",
    "# Display the result\n",
    "print(\"Final result:\")\n",
    "top_crimes.show(100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Execution time for SQL API: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Execution time for CSV import and SQL API: {end_time - start_time_csv:.2f} seconds\")\n",
    "# print(f\"Execution time for Parquet import and SQL API: {end_time - start_time_parquet:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined crime datasets successfully saved as a unique Parquet file to S3."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Combine Crime Datasets and Save to Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define S3 bucket path for the final Parquet file\n",
    "s3_bucket_path = \"s3://groups-bucket-dblab-905418150721/group34/Query_2/\"\n",
    "\n",
    "# Define the paths for the two crime datasets\n",
    "crime_data_path_1 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_path_2 = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Load both datasets (assuming CSV format; adjust for other formats if needed)\n",
    "crime_df1 = spark.read.csv(crime_data_path_1, header=True, inferSchema=True)\n",
    "crime_df2 = spark.read.csv(crime_data_path_2, header=True, inferSchema=True)\n",
    "\n",
    "# Combine the datasets (ensure schemas are aligned before union)\n",
    "combined_df = crime_df1.union(crime_df2)\n",
    "\n",
    "# Save the combined dataset as a single Parquet file\n",
    "combined_df.repartition(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(s3_bucket_path)\n",
    "\n",
    "print(\"Combined crime datasets successfully saved as a unique Parquet file to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "name": "SparkLab - Introduction to RDDs and DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
